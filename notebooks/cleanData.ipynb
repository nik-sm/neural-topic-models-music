{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import *\n",
    "init_notebook_mode(connected=True)\n",
    "import plotly.figure_factory as ff\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import enchant\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfC = pd.read_pickle(\"../data/lyrics_clean.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfC = dfC.drop([\"clean_lyrics\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stopwordsSet = stopwords.words(\"english\")\n",
    "d = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapAndDict(song):\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    tokens = tokenizer.tokenize(song.lower())\n",
    "    countDict = defaultdict(int)\n",
    "    stems = map(lambda x: (x, stemmer.stem(x)),[t for t in tokens if (not t in stopwordsSet)]) \n",
    "#     unusual = list([t for t in tokens if (not d.check(t))])\n",
    "#     print(unusual)\n",
    "    for tup in stems:\n",
    "        countDict[tup[1]] += 1\n",
    "    return countDict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "countDicts = map(lambda s: mapAndDict(s), dfC.sample(frac=50000.0/dfC.shape[0])[\"lyrics\"])\n",
    "sampleFrame = pd.DataFrame(list(countDicts)).to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleFrame.to_pickle(\"../data/sampleFrame50k.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedCounts = sampleFrame.sum(axis=0).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2kWords = sortedCounts[-2000:].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "topBagofWords = sampleFrame[top2kWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preacher       314.0\n",
       "bloom          314.0\n",
       "express        314.0\n",
       "sippin         314.0\n",
       "also           315.0\n",
       "dr             315.0\n",
       "pipe           315.0\n",
       "languag        315.0\n",
       "failur         315.0\n",
       "practic        316.0\n",
       "pari           316.0\n",
       "relat          316.0\n",
       "burst          316.0\n",
       "plug           317.0\n",
       "captain        317.0\n",
       "traffic        317.0\n",
       "w              317.0\n",
       "usual          317.0\n",
       "papa           318.0\n",
       "worship        318.0\n",
       "tour           318.0\n",
       "concern        319.0\n",
       "conscienc      319.0\n",
       "grin           319.0\n",
       "moan           319.0\n",
       "reveng         320.0\n",
       "mack           321.0\n",
       "chocol         321.0\n",
       "ole            321.0\n",
       "lame           322.0\n",
       "              ...   \n",
       "right        22759.0\n",
       "look         22891.0\n",
       "away         23378.0\n",
       "heart        23829.0\n",
       "need         23926.0\n",
       "life         25745.0\n",
       "day          25958.0\n",
       "back         28764.0\n",
       "yeah         29172.0\n",
       "caus         30224.0\n",
       "babi         31491.0\n",
       "way          31719.0\n",
       "take         32180.0\n",
       "say          32866.0\n",
       "feel         34901.0\n",
       "make         35952.0\n",
       "never        37713.0\n",
       "let          39376.0\n",
       "want         39665.0\n",
       "see          39771.0\n",
       "come         41940.0\n",
       "one          43261.0\n",
       "time         44661.0\n",
       "go           47567.0\n",
       "got          48260.0\n",
       "oh           49729.0\n",
       "get          54203.0\n",
       "like         67552.0\n",
       "know         69782.0\n",
       "love         74300.0\n",
       "Length: 2000, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topBagofWords.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "topBagofWords.to_pickle(\"../data/topBagofWords50k.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def unusual_words(text):\n",
    "#     text_vocab = set(w.lower() for w in text if w.isalpha())\n",
    "#     english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "#     unusual = text_vocab - english_vocab\n",
    "#     return sorted(unusual)\n",
    "\n",
    "for k in sampleFrame.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sampleFrame.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wordSet = set()\n",
    "dfC['stemmed'] = pd.Series(\"\", index=dfC.index)\n",
    "for i in range(dfC.shape[0]):\n",
    "    print(\"{}/{}\".format(i+1, dfC.shape[0]))\n",
    "    tokens = nltk.word_tokenize(dfC.iloc[i][\"clean_lyrics\"])\n",
    "    stems = [stemmer.stem(w) for w in tokens]\n",
    "    for s in stems:\n",
    "        wordSet.add(s)\n",
    "    dfC.at[i, \"stemmed\"] = \" \".join(stems)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(dfC[\"stemmed\"]) \n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemList = dfC[\"stemmed\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stemList[0].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([stemList[0]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xArr = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xArr[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(nltk.word_tokenize(stemList[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    stem = nltk.stem.SnowballStemmer('english')\n",
    "    text = text.lower()\n",
    "\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token in string.punctuation: continue\n",
    "        yield stem.stem(token)\n",
    "\n",
    "def vectorize(doc):\n",
    "    features = defaultdict(int)\n",
    "    for token in tokenize(doc):\n",
    "        features[token] += 1\n",
    "    return features\n",
    "\n",
    "vectors = map(vectorize, stemList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in vectors:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = defaultdict(int)\n",
    "d1[\"hello\"] = 1\n",
    "d2 = defaultdict(int)\n",
    "d2[\"dan\"] = 1\n",
    "pd.DataFrame([d1,d2]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(enchant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
